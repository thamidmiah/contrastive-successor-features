from collections import defaultdict, deque
from typing import Dict, List

import numpy as np
import torch

import global_context
import dowel_wrapper
from dowel import Histogram
from garage import TrajectoryBatch
from garage.misc import tensor_utils
from garage.np.algos.rl_algorithm import RLAlgorithm
from garagei import log_performance_ex
from garagei.torch.optimizers.optimizer_group_wrapper import OptimizerGroupWrapper
from garagei.torch.utils import compute_total_norm
from iod.utils import MeasureAndAccTime


class IOD(RLAlgorithm):
    def __init__(
            self,
            *,
            env_name,
            algo,
            env_spec,
            option_policy,
            traj_encoder,
            skill_dynamics,
            dist_predictor,
            dual_lam,
            optimizer,
            alpha,
            max_path_length,
            n_epochs_per_eval,
            n_epochs_per_log,
            n_epochs_per_tb,
            n_epochs_per_save,
            n_epochs_per_pt_save,
            n_epochs_per_pkl_update,
            dim_option,
            num_random_trajectories,
            num_video_repeats,
            eval_record_video,
            video_skip_frames,
            eval_plot_axis,
            name='IOD',
            device=torch.device('cpu'),
            sample_cpu=True,
            num_train_per_epoch=1,
            discount=0.99,
            sd_batch_norm=False,
            skill_dynamics_obs_dim=None,
            trans_minibatch_size=None,
            trans_optimization_epochs=None,
            discrete=False,
            unit_length=False,
    ):
        self.env_name = env_name
        self.algo = algo

        self.discount = discount
        self.max_path_length = max_path_length

        self.device = device
        self.sample_cpu = sample_cpu
        self.option_policy = option_policy.to(self.device)
        self.traj_encoder = traj_encoder.to(self.device)
        self.dual_lam = dual_lam.to(self.device)
        self.param_modules = {
            'traj_encoder': self.traj_encoder,
            'option_policy': self.option_policy,
            'dual_lam': self.dual_lam,
        }
        if skill_dynamics is not None:
            self.skill_dynamics = skill_dynamics.to(self.device)
            self.param_modules['skill_dynamics'] = self.skill_dynamics
        if dist_predictor is not None:
            self.dist_predictor = dist_predictor.to(self.device)
            self.param_modules['dist_predictor'] = self.dist_predictor

        self.alpha = alpha
        self.name = name

        self.dim_option = dim_option

        self._num_train_per_epoch = num_train_per_epoch
        self._env_spec = env_spec

        self.n_epochs_per_eval = n_epochs_per_eval
        self.n_epochs_per_log = n_epochs_per_log
        self.n_epochs_per_tb = n_epochs_per_tb
        self.n_epochs_per_save = n_epochs_per_save
        self.n_epochs_per_pt_save = n_epochs_per_pt_save
        self.n_epochs_per_pkl_update = n_epochs_per_pkl_update
        self.num_random_trajectories = num_random_trajectories
        self.num_video_repeats = num_video_repeats
        self.eval_record_video = eval_record_video
        self.video_skip_frames = video_skip_frames
        self.eval_plot_axis = eval_plot_axis

        assert isinstance(optimizer, OptimizerGroupWrapper)
        self._optimizer = optimizer

        self._sd_batch_norm = sd_batch_norm
        self._skill_dynamics_obs_dim = skill_dynamics_obs_dim

        if self._sd_batch_norm:
            self._sd_input_batch_norm = torch.nn.BatchNorm1d(self._skill_dynamics_obs_dim, momentum=0.01).to(self.device)
            self._sd_target_batch_norm = torch.nn.BatchNorm1d(self._skill_dynamics_obs_dim, momentum=0.01, affine=False).to(self.device)
            self._sd_input_batch_norm.eval()
            self._sd_target_batch_norm.eval()

        self._trans_minibatch_size = trans_minibatch_size
        self._trans_optimization_epochs = trans_optimization_epochs

        self.discrete = discrete
        self.unit_length = unit_length

        self.traj_encoder.eval()

    @property
    def policy(self):
        raise NotImplementedError()

    def all_parameters(self):
        for m in self.param_modules.values():
            for p in m.parameters():
                yield p

    def train_once(self, itr: int, paths: List[Dict[str, np.ndarray]], runner, extra_scalar_metrics: Dict = {}):
        logging_enabled = ((runner.step_itr + 1) % self.n_epochs_per_log == 0)

        data = self.process_samples(paths)

        time_computing_metrics = [0.0]
        time_training = [0.0]

        print(f"[DEBUG] train_once started for epoch {itr}")
        with MeasureAndAccTime(time_training):
            tensors = self._train_once_inner(data)
        print(f"[DEBUG] train_once completed for epoch {itr} in {time_training[0]:.2f}s")

        performence = log_performance_ex(
            itr,
            TrajectoryBatch.from_trajectory_list(self._env_spec, paths),
            discount=self.discount,
        )
        discounted_returns = performence['discounted_returns']
        undiscounted_returns = performence['undiscounted_returns']

        if logging_enabled:
            prefix_tabular = global_context.get_metric_prefix()
            with dowel_wrapper.get_tabular().prefix(prefix_tabular + self.name + '/'), dowel_wrapper.get_tabular(
                    'plot').prefix(prefix_tabular + self.name + '/'):
                def _record_scalar(key, val):
                    dowel_wrapper.get_tabular().record(key, val)

                def _record_histogram(key, val):
                    dowel_wrapper.get_tabular('plot').record(key, Histogram(val))

                for k in tensors.keys():
                    if tensors[k].numel() == 1:
                        _record_scalar(f'{k}', tensors[k].item())
                    else:
                        _record_scalar(f'{k}', np.array2string(tensors[k].detach().cpu().numpy(), suppress_small=True))
                with torch.no_grad():
                    # Log post-clip gradient norms captured inside _gradient_descent
                    # (Reading param.grad here would be 0 â€” grads are zeroed between steps)
                    if hasattr(self, '_last_grad_norm_after_clip') and self._last_grad_norm_after_clip:
                        total = sum(self._last_grad_norm_after_clip.values())
                        _record_scalar('TotalGradNormAll', total)
                        for key, norm in self._last_grad_norm_after_clip.items():
                            _record_scalar(f'TotalGradNorm{key.replace("_", " ").title().replace(" ", "")}', norm)
                    else:
                        _record_scalar('TotalGradNormAll', 0.0)
                
                # Log pre-clipping gradient norms to monitor if clipping is constantly engaged
                if hasattr(self, '_last_grad_norm_before_clip'):
                    for key, norm in self._last_grad_norm_before_clip.items():
                        _record_scalar(f'GradNormBeforeClip_{key}', norm)
                
                for k, v in extra_scalar_metrics.items():
                    _record_scalar(k, v)
                _record_scalar('TimeComputingMetrics', time_computing_metrics[0])
                _record_scalar('TimeTraining', time_training[0])

                path_lengths = [
                    len(path['actions'])
                    for path in paths
                ]
                _record_scalar('PathLengthMean', np.mean(path_lengths))
                _record_scalar('PathLengthMax', np.max(path_lengths))
                _record_scalar('PathLengthMin', np.min(path_lengths))

                _record_histogram('ExternalDiscountedReturns', np.asarray(discounted_returns))
                _record_histogram('ExternalUndiscountedReturns', np.asarray(undiscounted_returns))

        return np.mean(undiscounted_returns)

    def train(self, runner):
        last_return = None

        with global_context.GlobalContext({'phase': 'train', 'policy': 'sampling'}):
            for _ in runner.step_epochs(
                    full_tb_epochs=0,
                    log_period=self.n_epochs_per_log,
                    tb_period=self.n_epochs_per_tb,
                    pt_save_period=self.n_epochs_per_pt_save,
                    pkl_update_period=self.n_epochs_per_pkl_update,
                    new_save_period=self.n_epochs_per_save,
            ):
                for p in self.policy.values():
                    p.eval()
                self.traj_encoder.eval()

                if self.n_epochs_per_eval != 0 and runner.step_itr % self.n_epochs_per_eval == 0:
                    self._evaluate_policy(runner)

                for p in self.policy.values():
                    p.train()
                self.traj_encoder.train()

                for _ in range(self._num_train_per_epoch):
                    time_sampling = [0.0]
                    with MeasureAndAccTime(time_sampling):
                        runner.step_path: List[Dict[str, np.ndarray]] = self._get_train_trajectories(runner)
                    last_return = self.train_once(
                        runner.step_itr,
                        runner.step_path,
                        runner,
                        extra_scalar_metrics={
                            'TimeSampling': time_sampling[0],
                        },
                    )

                runner.step_itr += 1

        return last_return

    def _get_trajectories(self,
                          runner,
                          sampler_key,
                          batch_size=None,
                          extras=None,
                          update_stats=False,
                          worker_update=None,
                          env_update=None):
        if batch_size is None:
            batch_size = len(extras)
        policy_sampler_key = sampler_key[6:] if sampler_key.startswith('local_') else sampler_key
        time_get_trajectories = [0.0]
        with MeasureAndAccTime(time_get_trajectories):
            trajectories, infos = runner.obtain_exact_trajectories(
                runner.step_itr,
                sampler_key=sampler_key,
                batch_size=batch_size,
                agent_update=self._get_policy_param_values(policy_sampler_key),
                env_update=env_update,
                worker_update=worker_update,
                extras=extras,
                update_stats=update_stats,
            )
        print(f'_get_trajectories({sampler_key}) {time_get_trajectories[0]}s')

        for traj in trajectories:
            for key in ['ori_obs', 'next_ori_obs', 'coordinates', 'next_coordinates']:
                if key not in traj['env_infos']:
                    continue

        return trajectories

    def _get_train_trajectories(self, runner):
        default_kwargs = dict(
            runner=runner,
            update_stats=True,
            worker_update=dict(
                _render=False,
                _deterministic_policy=False,
            ),
            env_update=dict(_action_noise_std=None),
        )
        kwargs = dict(default_kwargs, **self._get_train_trajectories_kwargs(runner))

        paths = self._get_trajectories(**kwargs)

        return paths

    def process_samples(self, paths: List[Dict[str, np.ndarray]]) -> Dict[str, List[np.ndarray]]:
        """Process list of dictionaries into dictionary of lists.

        Args:
            paths (List[Dict[str, np.ndarray]]): List of dictionaries.

        Returns:
            Dict[str, List[np.ndarray]]: Dictionary of lists.
        """
        data: Dict[str, List[np.ndarray]] = defaultdict(list)
        for path in paths:
            # data['coordinates'].append(path['env_infos']['coordinates'])
            # data['next_coordinates'].append(path['env_infos']['next_coordinates'])
            data['obs'].append(path['observations'])
            data['next_obs'].append(path['next_observations'])
            # data['final_obs'].append(path['next_observations'][-1::, ...].repeat(path['next_observations'].shape[0], axis=0))
            # data['initial_obs'].append(path['observations'][0:1, ...].repeat(path['observations'].shape[0], axis=0))
            data['actions'].append(path['actions'])
            data['next_actions'].append(np.concatenate((path['actions'][1:, ...], np.zeros_like(path['actions'][0])[None]), axis=0))
            data['rewards'].append(path['rewards'])
            data['dones'].append(path['dones'])
            data['returns'].append(tensor_utils.discount_cumsum(path['rewards'], self.discount))
            if 'ori_obs' in path['env_infos']:
                data['ori_obs'].append(path['env_infos']['ori_obs'])
            if 'next_ori_obs' in path['env_infos']:
                data['next_ori_obs'].append(path['env_infos']['next_ori_obs'])
            # data['final_obs'].append(path['next_observations'][-1, ...])
            # data['initial_obs'].append(path['observations'][0, ...])
            if 'pre_tanh_value' in path['agent_infos']:
                data['pre_tanh_values'].append(path['agent_infos']['pre_tanh_value'])
            if 'log_prob' in path['agent_infos']:
                data['log_probs'].append(path['agent_infos']['log_prob'])
            if 'option' in path['agent_infos']:
                data['options'].append(path['agent_infos']['option'])
                data['next_options'].append(np.concatenate([path['agent_infos']['option'][1:], path['agent_infos']['option'][-1:]], axis=0))

        return data

    def _get_policy_param_values(self, key):
        param_dict = self.policy[key].get_param_values()
        for k in param_dict.keys():
            if self.sample_cpu:
                param_dict[k] = param_dict[k].detach().cpu()
            else:
                param_dict[k] = param_dict[k].detach()
        return param_dict

    def _generate_option_extras(self, options):
        return [{'option': option} for option in options]

    def _gradient_descent(self, loss, optimizer_keys, max_grad_norm=10.0):
        # CRITICAL: Clear ALL gradients first to prevent accumulation across optimizer steps
        # This is essential because we may have shared modules (e.g., CNN encoder) that
        # get gradients from multiple optimizer steps (qf, option_policy, traj_encoder)
        # but aren't included in every optimizer_keys set.
        for param in self.all_parameters():
            param.grad = None
        
        # Now compute gradients for this specific loss
        loss.backward()
        
        # CRITICAL: Gradient clipping for stability
        # Clip ALL parameters that have gradients (prevents critic divergence)
        if max_grad_norm is not None and max_grad_norm > 0:
            # Collect all parameters that were just updated (have gradients)
            params_to_clip = [p for p in self.all_parameters() if p.grad is not None]
            
            if params_to_clip:
                # Clip gradients in-place and return the total norm BEFORE clipping
                total_norm_before = torch.nn.utils.clip_grad_norm_(params_to_clip, max_grad_norm)
                
                # Store the pre-clipping norm for diagnostic logging
                # This will help us monitor if we're constantly clipping (sign that LR is too high)
                if not hasattr(self, '_last_grad_norm_before_clip'):
                    self._last_grad_norm_before_clip = {}
                # Store by optimizer key for tracking
                key_str = '_'.join(sorted(optimizer_keys)) if isinstance(optimizer_keys, list) else str(optimizer_keys)
                self._last_grad_norm_before_clip[key_str] = total_norm_before.item()

                # Store per-module post-clip gradient norms (captured NOW, before they're zeroed)
                if not hasattr(self, '_last_grad_norm_after_clip'):
                    self._last_grad_norm_after_clip = {}
                for key in optimizer_keys:
                    if key in self.param_modules:
                        norm = compute_total_norm(self.param_modules[key].parameters())
                        self._last_grad_norm_after_clip[key] = norm.item()
                    else:
                        # Handle optimizer keys that map to multiple param_modules
                        # e.g. 'qf' optimizer covers both 'qf1' and 'qf2' modules
                        for pm_key, module in self.param_modules.items():
                            if pm_key.startswith(key):
                                norm = compute_total_norm(module.parameters())
                                self._last_grad_norm_after_clip[pm_key] = norm.item()
        
        # Step only the optimizers specified (their params now have clipped grads)
        self._optimizer.step(keys=optimizer_keys)

    def _get_mini_tensors(self, epoch_data):
        num_transitions = len(epoch_data['actions'])
        idxs = np.random.choice(num_transitions, self._trans_minibatch_size)

        data = {}
        for key, value in epoch_data.items():
            data[key] = value[idxs]

        return data

    def _log_eval_metrics(self, runner):
        runner.eval_log_diagnostics()
        runner.plot_log_diagnostics()
